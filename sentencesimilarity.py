# -*- coding: utf-8 -*-
"""sentencesimilarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SHPpcDgEuKiOUznSIWrHTwvs35EccGN-
"""

from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import numpy as np
threshold = 0.60

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)

sim_matrix = cosine_similarity(embeddings)

def get_representative_sentences(sentences, embeddings, threshold=0.75,n=2):
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    sim_matrix = cosine_similarity(embeddings)
    n = len(sentences)
    visited = [False] * n
    clusters = []

    def dfs(i, cluster):
        visited[i] = True
        cluster.append(i)
        for j in range(n):
            if not visited[j] and sim_matrix[i][j] >= threshold:
                dfs(j, cluster)

    # Form clusters
    for i in range(n):
        if not visited[i]:
            cluster = []
            dfs(i, cluster)
            clusters.append(cluster)

    # Extract representative sentences for valid clusters
    representatives = []
    for cluster in clusters:
        if len(cluster) >= n:
            cluster_embeddings = [embeddings[i] for i in cluster]
            cluster_sentences = [sentences[i] for i in cluster]

            centroid = np.mean(cluster_embeddings, axis=0)
            similarities = cosine_similarity([centroid], cluster_embeddings)[0]

            rep_idx = np.argmax(similarities)
            representative = cluster_sentences[rep_idx]
            representatives.append(representative)

    return representatives

get_representative_sentences(sentences,embeddings)

# n = len(sentences)
# visited = [False] * n
# clusters = []

# def dfs(i, cluster):
#     visited[i] = True
#     cluster.append(i)
#     for j in range(n):
#         if not visited[j] and sim_matrix[i][j] >= threshold:
#             dfs(j, cluster)

# # Form clusters using DFS
# for i in range(n):
#     if not visited[i]:
#         cluster = []
#         dfs(i, cluster)
#         clusters.append(cluster)

# # Print clusters
# for idx, cluster in enumerate(clusters):
#     print(f"\nCluster {idx + 1}:")
#     for i in cluster:
#         print(f"  - {sentences[i]}")

str="This is an example sentence"
len()

!pip install keybert

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

ARTICLE = """ the audio was not clear
"""
print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))

# for idx, cluster in enumerate(clusters):
#     cluster_embeddings = [embeddings[i] for i in cluster]
#     cluster_sentences = [sentences[i] for i in cluster]

#     centroid = np.mean(cluster_embeddings, axis=0)
#     similarities = cosine_similarity([centroid], cluster_embeddings)[0]

#     # Find the sentence closest to the centroid
#     rep_idx = np.argmax(similarities)
#     representative = cluster_sentences[rep_idx]

#     print(f"\nğŸ”· Cluster {idx + 1} (Total Sentences: {len(cluster)}):")
#     print(f"  ğŸ· Representative: {representative}")
#     print("  ğŸ§¾ Sentences:")
#     for s in cluster_sentences:
#         print(f"    - {s}")